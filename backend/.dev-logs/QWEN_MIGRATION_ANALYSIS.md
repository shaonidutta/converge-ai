# Qwen LLM Migration Analysis

**Date**: 2025-10-14  
**Current LLM**: Gemini 2.0 Flash (via LangChain Google GenAI)  
**Target LLM**: Qwen (Alibaba Tongyi)  
**Purpose**: Analyze changes needed to switch from Gemini to Qwen

---

## üìä **EXECUTIVE SUMMARY**

**Minimal Changes Required**: Only **1 file** needs modification (PolicyAgent)  
**Estimated Time**: 30-60 minutes  
**Complexity**: LOW  
**Breaking Changes**: None (same LangChain interface)

---

## üîç **CURRENT IMPLEMENTATION**

### **File**: `backend/src/agents/policy/policy_agent.py`

**Current LLM Setup** (Lines 16, 52-57):
```python
from langchain_google_genai import ChatGoogleGenerativeAI

self.llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key=settings.GOOGLE_API_KEY,
    temperature=0.1,
    max_tokens=1024
)
```

**Usage** (Line 368):
```python
response = self.llm.invoke([system_message, human_message])
```

---

## üéØ **QWEN INTEGRATION OPTIONS**

### **Option 1: Alibaba Cloud API (Recommended for Production)**

**Pros**:
- ‚úÖ Cloud-hosted, no local GPU needed
- ‚úÖ Similar to Gemini API (API key based)
- ‚úÖ Built-in LangChain support via `langchain_community`
- ‚úÖ No rate limits like Gemini free tier
- ‚úÖ Fast response times
- ‚úÖ Scalable

**Cons**:
- ‚ùå Requires Alibaba Cloud account
- ‚ùå Paid service (but cheaper than Gemini)

**Implementation**:
```python
from langchain_community.chat_models import ChatTongyi

self.llm = ChatTongyi(
    model="qwen-turbo",  # or qwen-plus, qwen-max
    dashscope_api_key=settings.ALIBABA_API_KEY,
    temperature=0.1,
    max_tokens=1024
)
```

**Changes Needed**:
1. Install: `pip install dashscope`
2. Add to `.env`: `ALIBABA_API_KEY=your_key_here`
3. Update `settings.py`: Add `ALIBABA_API_KEY` field
4. Update `policy_agent.py`: Change import and initialization (5 lines)

**Total Changes**: 4 files, ~10 lines of code

---

### **Option 2: Local Qwen Model (Self-Hosted)**

**Pros**:
- ‚úÖ No API costs
- ‚úÖ No rate limits
- ‚úÖ Full control
- ‚úÖ Data privacy

**Cons**:
- ‚ùå Requires GPU (8GB+ VRAM for Qwen2.5-7B)
- ‚ùå Slower inference
- ‚ùå More complex setup
- ‚ùå Need to manage model files

**Implementation**:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.llms.base import LLM
from typing import Optional, List

class QwenLocal(LLM):
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"
    temperature: float = 0.1
    max_tokens: int = 1024
    
    def __init__(self):
        super().__init__()
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
    
    @property
    def _llm_type(self) -> str:
        return "qwen-local"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional = None,
    ) -> str:
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=self.max_tokens,
            temperature=self.temperature
        )
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

# Usage
self.llm = QwenLocal()
```

**Changes Needed**:
1. Install: `pip install transformers torch accelerate`
2. Create `qwen_local.py` with custom LLM class (~80 lines)
3. Update `policy_agent.py`: Import and use QwenLocal
4. Download model files (~14GB for Qwen2.5-7B)

**Total Changes**: 2 files, ~85 lines of code

---

### **Option 3: Ollama + Qwen (Local with Easy Setup)**

**Pros**:
- ‚úÖ Easy local setup
- ‚úÖ No API costs
- ‚úÖ Built-in LangChain support
- ‚úÖ Model management handled by Ollama

**Cons**:
- ‚ùå Requires GPU
- ‚ùå Need to install Ollama
- ‚ùå Slower than cloud API

**Implementation**:
```python
from langchain_community.llms import Ollama

self.llm = Ollama(
    model="qwen2.5:7b",
    temperature=0.1,
    num_predict=1024
)
```

**Changes Needed**:
1. Install Ollama: https://ollama.ai/download
2. Pull model: `ollama pull qwen2.5:7b`
3. Install: `pip install langchain-ollama`
4. Update `policy_agent.py`: Change import and initialization (5 lines)

**Total Changes**: 1 file, ~5 lines of code

---

## üìù **DETAILED MIGRATION STEPS**

### **For Option 1 (Alibaba Cloud API - RECOMMENDED)**

#### **Step 1: Install Dependencies**
```bash
pip install dashscope langchain-community
```

#### **Step 2: Update `.env`**
```env
# Add this line
ALIBABA_API_KEY=your_dashscope_api_key_here
```

#### **Step 3: Update `backend/src/core/config/settings.py`**
```python
# Add this field to Settings class
ALIBABA_API_KEY: str = Field(..., env="ALIBABA_API_KEY")
```

#### **Step 4: Update `backend/src/agents/policy/policy_agent.py`**

**Change Line 16**:
```python
# FROM:
from langchain_google_genai import ChatGoogleGenerativeAI

# TO:
from langchain_community.chat_models import ChatTongyi
```

**Change Lines 52-57**:
```python
# FROM:
self.llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key=settings.GOOGLE_API_KEY,
    temperature=0.1,
    max_tokens=1024
)

# TO:
self.llm = ChatTongyi(
    model="qwen-turbo",  # Options: qwen-turbo, qwen-plus, qwen-max
    dashscope_api_key=settings.ALIBABA_API_KEY,
    temperature=0.1,
    max_tokens=1024
)
```

**That's it!** No other changes needed.

---

## üîÑ **COMPATIBILITY ANALYSIS**

### **What Stays the Same**
‚úÖ **LangChain Interface**: Both use same `invoke()` method  
‚úÖ **Message Format**: Both support SystemMessage and HumanMessage  
‚úÖ **Parameters**: temperature, max_tokens work the same  
‚úÖ **Response Format**: Both return string responses  
‚úÖ **Async Support**: Both support async operations  
‚úÖ **Streaming**: Both support streaming (if needed)  

### **What Changes**
üîÑ **Import Statement**: Different package  
üîÑ **Class Name**: ChatGoogleGenerativeAI ‚Üí ChatTongyi  
üîÑ **API Key Parameter**: google_api_key ‚Üí dashscope_api_key  
üîÑ **Model Names**: gemini-2.0-flash-exp ‚Üí qwen-turbo/plus/max  

### **What Might Need Adjustment**
‚ö†Ô∏è **Prompt Engineering**: Qwen may respond differently to same prompts  
‚ö†Ô∏è **Response Quality**: May need to test and adjust prompts  
‚ö†Ô∏è **Rate Limits**: Different rate limit structure  
‚ö†Ô∏è **Error Handling**: Different error types  

---

## üí∞ **COST COMPARISON**

### **Gemini 2.0 Flash (Current)**
- **Free Tier**: 10 requests/minute
- **Paid**: $0.075 per 1M input tokens, $0.30 per 1M output tokens
- **Issue**: Rate limits causing test failures

### **Qwen via Alibaba Cloud**
- **qwen-turbo**: ¬•0.0008/1K tokens (~$0.11/1M tokens)
- **qwen-plus**: ¬•0.004/1K tokens (~$0.55/1M tokens)
- **qwen-max**: ¬•0.04/1K tokens (~$5.50/1M tokens)
- **Rate Limits**: Much higher (100+ requests/minute)

**Recommendation**: Start with **qwen-turbo** (cheapest, faster than Gemini free tier)

---

## üéØ **MIGRATION CHECKLIST**

### **Pre-Migration**
- [ ] Get Alibaba Cloud account
- [ ] Get DashScope API key
- [ ] Test API key with simple request
- [ ] Backup current code

### **Migration**
- [ ] Install dependencies (`dashscope`, `langchain-community`)
- [ ] Update `.env` with ALIBABA_API_KEY
- [ ] Update `settings.py` with new field
- [ ] Update `policy_agent.py` import and initialization
- [ ] Update docstrings mentioning "Gemini"

### **Testing**
- [ ] Run simple test query
- [ ] Run manual test script
- [ ] Compare response quality with Gemini
- [ ] Adjust prompts if needed
- [ ] Run comprehensive test suite (40 tests)
- [ ] Verify grounding scores

### **Post-Migration**
- [ ] Update documentation
- [ ] Commit changes
- [ ] Update README with new setup instructions
- [ ] Monitor API usage and costs

---

## üìä **EXPECTED RESULTS**

### **Benefits**
‚úÖ **No Rate Limits**: Can run all 40 tests without delays  
‚úÖ **Lower Cost**: ~85% cheaper than Gemini paid tier  
‚úÖ **Better for Chinese**: Qwen optimized for Chinese language  
‚úÖ **Faster Testing**: No need to wait between requests  

### **Potential Issues**
‚ö†Ô∏è **Response Quality**: May differ from Gemini (need testing)  
‚ö†Ô∏è **Prompt Tuning**: May need to adjust prompts  
‚ö†Ô∏è **Learning Curve**: New API to learn  

---

## üöÄ **RECOMMENDATION**

**Use Option 1 (Alibaba Cloud API)** because:

1. **Minimal Code Changes**: Only 1 file, ~10 lines
2. **No Infrastructure**: No GPU needed
3. **No Rate Limits**: Can run all tests
4. **Cost Effective**: Cheaper than Gemini
5. **Production Ready**: Cloud-hosted, scalable
6. **Easy Rollback**: Can switch back easily

**Estimated Migration Time**: 30-60 minutes  
**Risk Level**: LOW  
**Effort**: MINIMAL  

---

## üìù **SUMMARY**

| Aspect | Current (Gemini) | After (Qwen Cloud) | Change |
|--------|------------------|-------------------|--------|
| **Files Modified** | - | 1 | +1 |
| **Lines Changed** | - | ~10 | +10 |
| **Dependencies** | langchain-google-genai | dashscope, langchain-community | +2 |
| **API Key** | GOOGLE_API_KEY | ALIBABA_API_KEY | +1 |
| **Rate Limit** | 10/min | 100+/min | +10x |
| **Cost** | $0.075-0.30/1M | $0.11/1M | -73% |
| **Migration Time** | - | 30-60 min | - |
| **Risk** | - | LOW | - |

**Conclusion**: Switching to Qwen requires **minimal changes** (1 file, ~10 lines) and provides **significant benefits** (no rate limits, lower cost). Highly recommended for this project.

